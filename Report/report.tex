\documentclass[12pt]{article}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{cite}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usepackage{multirow}
\graphicspath{{../figs/}}
% Include Code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Include Algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage[margin=1in]{geometry}
\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

% Typeset Shortcuts
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\F}{{\mathcal{F}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}

\DeclareMathOperator{\indep}{\perp\hspace{-.6em}\perp}

  \usepackage{tcolorbox}
  \newcommand{\summary}[1]{
  	\vspace{0.25cm}
  	\begin{tcolorbox}
  		\small{
  			%\textbf{Summary.}
  			#1}
  	\end{tcolorbox}
  }

\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\Norm}[1]{\left\|{#1}\right\|}
\newcommand{\rank}{{\mathrm{rank}}}
\newcommand{\Var}{\mathrm{Var}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\I}{{\mathbb{I}}}
\newcommand{\diag}{{\mathrm{diag}}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\eps}{\varepsilon}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\Conv}{\mathrm{Conv}}
\newcommand{\Int}{\mathrm{int}}
\newcommand{\heart}{\heartsuit}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\begin{document}
\begin{titlepage}
	\centering
	
	{\scshape \Large CPSC 538R Project Report
\par}
	\vspace{1cm}
	{\huge\bfseries Lifted Parallel Tempering for Training Restricted Boltzmann Machines
\par}
	\vspace{0.5cm}
	{\Large  Amit Kadan, Saifuddin Syed, Esten Nicolai W{\o}ien
\par}

	\vfill
	Submitted to\par
	{\scshape Dr.~Siamak Ravanbakhsh}\\
		{ \today\par}


% Bottom of the page

\end{titlepage}

\tableofcontents
\newpage

\input{sec_1_intro}
\section{Restricted Boltzmann Machines}
\input{sec_2_RBM}

\section{Contrastive Divergence}
\input{sec_3_CD}
\section{Tempering Methods}
\input{sec_4_PT}
\subsection{PT for RBM Training}
We now outline how we use parallel tempering for training RBM's. PCD is essentially performing Gibbs on \eqref{RBM_gibbs} while updating the gradients at each iteration. We can adapt the parallel tempering framework to improve the learning in PCD by allowing multiple Gibbs chains to run simultaneously, and performing swaps using the rules of PT, LPT, and LPTD to help improve the mixing in Gibbs sampling. Similar to PCD, we continue to update the gradient via \eqref{LL_grad} after each iteration. 

Desjardins \cite{desjardins2010tempered} shows that a parallel tempering version of PCD achieves much better mixing than the standard PCD. For the MNIST dataset, PT has a drastic reduction in correlation between successive samples. This results in a much more efficient sampler that's able to sample from the different modes without time-consuming exploration. This is an important improvement to PCD since the running time rapidly increases with the complexity of the model and size of dataset.

On constructed datasets where the underlying distribution has modes that are highly separated, PT outperfroms both CD, PCD \cite{desjardins2010tempered}. Similar results are shown by \cite{salakhutdinov2009tempered}. Desjardins also demonstrates how the improved mixing properties of PT results in larger optimal learning rate compared to CD, PCD.

In a learning scenario with large datasets and complex models, it's often unfeasible to reach convergence of log-likelihood. In such cases any improvement of convergence rate will result in an improved log-likelihood for a fixed number of learning iterations. Desjardins \cite{desjardins2010tempered} shows that this applies to training RBMs with PT. Using this as the training procedure gains a large increase in log-likelihood compared to CD, and PCD. However, using the last PT sample as initial state when sampling does not seem to affect the quality of the samples. Initializing from the test set, or randomly initialization has equal performance. This indicates that having a more accurate distribution is more important than an accurate sampling procedure.

\section{Experiments}
\input{sec_5_experiments}
\bibliography{01} 
\bibliographystyle{alpha}
%\section{Code}

\end{document}
