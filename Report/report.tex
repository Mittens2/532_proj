\documentclass[12pt]{article}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{cite}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usepackage{multirow}
\graphicspath{{../figs/}}
% Include Code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Include Algorithm
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage[margin=1in]{geometry}
\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

% Typeset Shortcuts
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\F}{{\mathcal{F}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}

\DeclareMathOperator{\indep}{\perp\hspace{-.6em}\perp}

  \usepackage{tcolorbox}
  \newcommand{\summary}[1]{
  	\vspace{0.25cm}
  	\begin{tcolorbox}
  		\small{
  			%\textbf{Summary.}
  			#1}
  	\end{tcolorbox}
  }

\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\Norm}[1]{\left\|{#1}\right\|}
\newcommand{\rank}{{\mathrm{rank}}}
\newcommand{\Var}{\mathrm{Var}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\I}{{\mathbb{I}}}
\newcommand{\diag}{{\mathrm{diag}}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\eps}{\varepsilon}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\Conv}{\mathrm{Conv}}
\newcommand{\Int}{\mathrm{int}}
\newcommand{\heart}{\heartsuit}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\begin{document}
\begin{titlepage}
	\centering
	
	{\scshape \Large CPSC 538R Project Report
\par}
	\vspace{1cm}
	{\huge\bfseries Non-Reversible Parallel Tempering for Training Restricted Boltzmann Machines
\par}
	\vspace{0.5cm}
	{\Large  Amit Kadan, Saifuddin Syed, Esten Nicolai W{\o}ien
\par}

	\vfill
	Submitted to\par
	{\scshape Dr.~Siamak Ravanbakhsh}\\
		{ \today\par}


% Bottom of the page

\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}
Restricted Boltzmann Machines (RBMs) are Markov random fields with applications including density estimation, 
dimensionality reduction and collaborative filtering. 

\section{Restricted Boltzmann Machines}
\input{sec_2_RBM}

\section{Contrastive Divergence}
\input{sec_3_CD}
\section{Tempering Methods}
\input{sec_4_PT}

\subsection{PT for RBM Training}
We now outline how we use parallel tempering for training RBM's. PCD is essentially perform Gibbs on \eqref{RBM_gibbs} while updating the gradients at each iteration. We can adapt the parallel tempering framework to improve the learning in PCD by allowing multiple Gibbs chains to run simultaneously perform swaps as using the rules of PT, LPT, LPTD to help improve the mixing in Gibbs sampling. Similar to PCD, we continue to update the gradient via \eqref{LL_grad} after each iteration. 

Desjardins \cite{desjardins2010tempered} shows that a parallel tempering version of PCD achieves much better mixing than the standard PCD. For the MNIST dataset, PTPCS hase a drastic reduction in correlation between successive samples. This results in a much more efficient sampler that's able to sample from the different modes without the need of a time-consuming exploration. This is an important improvement to PCD since the running time rapidly increases with the complexity of the model and size of dataset.

On constructed datasets where the underlying distribution has modes that are highly separated, both CD, PCD shows substantial decreases in log-likelihood \cite{desjardins2010tempered}. However, the log-likelihood for PT seem to converge without any substantial decrease. Similar results are shown by \cite{salakhutdinov2009tempered}. Desjardins also demonstrates how the improved mixing properties of PT results in larger optimal learning rate compared to CD, PCD.

In a learning scenario with large datasets and complex models, it's often unfeasible to reach convergence of log-likelihood. In such cases any improvement of convergence rate will result in an improved log-likelihood for a fixed number of learning iterations. Desjardins \cite{desjardins2010tempered} shows that this applies to training RBMs with PT. Using this as the training procedure gains a large increase in log-likelihood compared to CD, PCD. However, using the last PT sample as initial state when sampling does not seem to affect the quality of the samples. Initializing from the test set, or randomly initialization has equal performance. This indicates that having a more accurate distribution is more important than an accurate sampling procedure.

\section{Experiments}
\input{sec_5_experiments}
\bibliography{01} 
\bibliographystyle{alpha}
%\section{Code}

\end{document}
